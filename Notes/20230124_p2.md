# An Information Minimization Based Contrastive Learning Model for Unsupervised Sentence Embeddings Learning

https://arxiv.org/pdf/2209.10951.pdf

#### 任务：

sentence representation

#### 他的方法：

<img src="https://p.ipic.vip/bq2oio.png" alt="p2" width="600"/>

* 我们原来的contrastive learning其实是一种mutual information的一种lower bound，但其实包含了很多没有用的信息，比如stopwords，其实多余的可能还有bias
* 那么我们其实要在maximize mutual information的时候同时minimize information entropy
  * $H(Z|S)=logP(Z_1|Z_2)=logP_{Gaussian}(Z1|Z2)=||z_1-z_2||^2$
  * 一开始我们想最小化熵，那么其实我们想要的是reconstruction，而基于hidden label的reconstruction，我们都可以假设它服从了高斯分布，那么我们其实只要让hidden做L2 distance就可以了

