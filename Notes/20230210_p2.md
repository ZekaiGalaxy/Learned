# Inducer-tuning: Connecting Prefix-tuning and Adapter-tuning

https://arxiv.org/pdf/2210.14469.pdf

### 他的方法：

* 其实adapter tuning和prefix tuning是一样的
* 我们可以把prefix看成是参数前面加了个小前缀，然后重点研究attention模块
* 这就相当于改变了K和V，Q的改变暂时不考虑
* 然后带进公式里推几步就发现，其实就是一个adapter
* 但是这样来看，其实有一些limitation
  * 因为我们得到Q和K其实都要经过proj，那么proj的时候这个几何关系其实是确定不变的，没有做到adaptive，也没有做到task specific
  * “pairwise positional interactions”
