# ABC: Attention with Bounded-Memory Control

https://arxiv.org/pdf/2110.02488.pdf

#### 任务：

改进attention

#### 他的方法：

把attention看成memory network
* $\\{q_1,q_2...q_N\\},q_i \to \\{(k_i,v_i)\\}$
* 在传统attention中，每个qi都要把每个slot全给看一遍
* 但是可以有一些变化
  * sliding window，每个qi只能看window size个slot，然后q和这window size个key进行乘法
  * key cluster，每次只看一个cluster中的key，可以用01矩阵乘以key来表示
  * 因此我们其实可以不让query全看到，而是可以加个mask指定它看什么
  * 这个mask可以根据query生成，$\alpha_i=W_qq_i$，然后alpha_i在行间进行softmax，来“控制”attention看什么
* 其他attention的分类
  * context-agnostic
  * context-dependent/aware

