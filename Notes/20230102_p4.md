# Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning

https://arxiv.org/pdf/2109.05687.pdf

#### 任务：

大模型小数据finetune，参数太大造成过拟合

#### 其他方法：

用pretrained weight去regularize

* Weight Decay
  * $L(w) = L_{CE}(w) + λ_{WD}\lVert kw − w_0\lVert_2$
* Top-K Fine-tuning
  * top k layers
* Mixout
  * 用pretrained weight替代一部分
* RecAdam
  * 对weight decay改编
* Robust Representations through Regularized Fine-tuning (R3F) 
  * $KL(f(x)||f(x+noise))$来约束

#### 他的方法：

* 只用一些参数
  * task free
    * Bernoulli(p)
  * task driven
    * "重要的参数"
    * Fisher Information Matrix
  * 实现：在某些梯度上乘以mask

<img src="https://p.ipic.vip/w3b4nq.png" alt="p2" width="300"/>
