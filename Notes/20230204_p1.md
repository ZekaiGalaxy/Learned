# Flooding-X: Improving BERT’s Resistance to Adversarial Attacks via Loss-Restricted Fine-Tuning

https://aclanthology.org/2022.acl-long.386.pdf

### 任务：

prevent overfitting

### 他的方法：

<img src="https://p.ipic.vip/mymuyr.png" alt="p2" width="400"/>

<img src="https://p.ipic.vip/4t9djv.png" alt="p2" width="400"/>

* 当loss下降到一定程度我们就不下降了，而是开始维持这个水平的loss
* 怎么做到维持？
  * 我们可以alternatively做gradient descent & gradient ascent
  * 这其实是另一种训练范式，优化的目标变成了梯度的平方
* 到什么时候开始维持了？
  * 我们可以研究如果某个梯度下降了会对其他的loss有什么影响
  * 研究delta loss before/after gradient descent
    * 发现其实是gradient的accordance，两个gradient方向差不多，那么就是正影响，否则就是negative impact
    * 最重要的是taylor！
      * $f(x+y,z)=f(x,z)+\frac {\partial f}{\partial x}z$
  * 于是我们就可以观察batch之间的grad accordance，class之间的grad accordance来判断
