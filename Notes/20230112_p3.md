# Attention Temperature Matters in Abstractive Summarization Distillation

https://arxiv.org/pdf/2106.03441.pdf

#### 任务：

知识蒸馏

一般方法：

* soft targets，intermediate hidden states，attentions ，tar- get output derivatives

#### 他的方法：

* seq2seq基本上都用的是pseudo label来蒸馏
* 他发现teacher model的attention分布集中在前面，且会copy，有copy bias和leading bias
* 用rescaling attention的方法，让原本很sharp的attention变软一点，这样也更适合教学
* $Attn(Q,K,V)=softmax(QK^t/\lambda\tau)V$，在train的时候不变；在inference的时候调整lambda
* 可以解决copy bias和leading bias，更abstractive，虽然生成效果也更差
