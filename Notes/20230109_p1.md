# Linearizing Transformer with Key-Value Memory

https://arxiv.org/pdf/2203.12644.pdf

#### 任务：

设计一种新的transformer，把attention换掉，这样计算效率更高

#### 他的方法：

memory network

* SA是 $x^{out} = softmax(qK^T)V$，其中K和V都靠投影
* 我们现在要设计memory，假设有K个memory slot
  * $x^{out} = softmax(qM_k^T)V_k$，其中M_k就是一个固定的有k个slot的matrix
  * V_k是把x通过线性层转换成一个k维向量，相当于某个对应的key会怎么处理这个input，关注并提取哪些信息
  * 固定了K，这是input-independent的，我们要学习的就是如何让q和这个matrix align，如何更好的符合slot
