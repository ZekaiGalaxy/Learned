# Multi-Granularity Structural Knowledge Distillation for Language Model Compression

https://aclanthology.org/2022.acl-long.71.pdf

#### 任务：

知识蒸馏

#### 别人方法：

* Which, what, how
  * Which granularity
  * what form of knowledge is easy to transfer
  * how to teach the student using knowledge
* contrastive distillation: teacher as positive
* attention dependency

#### 他的方法：

<img src="https://p.ipic.vip/6vcrym.png" alt="p2" width="600"/>

* multi-granularity
  * token
    * Workpiece
  * span
    * spanbert
      * random length, random start token
    * Workpiece: extract NP,VP,PP
  * sample
* Structural relation： Geometrical
  * Pair-wise
  * Triplet-wise
    * angle loss
  * 我们可以把hidden先进行projection再在subspaces中进行loss
  * 但是我们要进行重要性筛选，可以根据self attention中的score作为saliency score
* Hierachical
  * bottom for syntatic
  * upper for semantic

