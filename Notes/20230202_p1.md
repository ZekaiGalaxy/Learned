# Contextual Representation Learning beyond Masked Language Modeling

https://arxiv.org/pdf/2204.04163.pdf

### 任务：

sentence repr

### 他的方法：

<img src="https://p.ipic.vip/zxuvwr.png" alt="p2" width="400"/>

* observation
  * 普通的MLM repr靠的是masked token repr来anchor learned sentence repr
* 这样通过sampled token来align的，都是local bias，提出要加一个global bias
  * g = h-e
  * 可以把global bias表达成hidden-original embedding
  * 用INfoNCE
    * positive用nearby token in same sentence
    * negative用token in other sentence
    * 其实InfoNCE就是在优化互信息，只是一个lower bound而已
      * $I(x,y) \ge log(K)-L_{InfoNCE}(x,y)$
