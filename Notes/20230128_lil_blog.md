https://lilianweng.github.io/posts/



# Contrastive Representation Learning

* loss
  * triplet loss
  * n-pair loss
  * lifted structured loss
    * 所有pair之间的相似度都用到

<img src="https://p.ipic.vip/6r86rb.png" alt="p2" width="400"/>

* tricks

  * heavy DA
  * large batch size
  * hard negative
    * NLI
    * Most keyword match (BM25)
    * memory bank
    * 其中如果有false negative，那么效果会drop很多
    * “修正”negative sampling的概率

* image DA

  * mixup：pixel wise mixture
  * cutmix： region wise mixture
  * MoCHi：用q去sort negative，来控制hard的程度
    * negative之间interpolation
    * 和q之间interpolation

* text DA

  * Synonym replacement (SR)
  * Random insertion (RI)
  * Random swap (RS)
  * Random deletion (RD)
  * Back Translation (BT)
  * Dropout, cutoff

* Barlow Twins

  <img src="https://p.ipic.vip/ydsyxq.png" alt="p2" width="400"/>

  * 利用identity matrix来作为label
  * 既考虑relevance，又考虑redundancy
  * 你还可以考虑的是mutual information (max or min)

* BYOL

  * 你可以在学习了的和没有经过学习的repr之间进行对比学习
  * bootstrap learning
  * 其实关键在于batch normalization，negative sample的作用其实是让模型不至于塌缩，让模型记住一些bias，而这正是batch normalization的作用

* Memory bank

  * momentum contrast
  * 可以用队列维护samples，也可以构造出一快一慢的encoder
  * MLP很有用！！

* swap

  * 让一个augmented生成z，让另一种augmented的生成它



# Semi-Supervised Learning

* self training
  * convert the most confident samples to label
  * 关键在于reduce confimation bias
* mixmatch
  * 一种consistency regularization
  * 让模型更加自信
    * sharpening logits
    * entropy minimization
  * Remixmatch 一些改进
    * 让模型predict的label的分布要求和gt的分布一样（一些“argue”）
  * Fixmatch
    * 只有confident才加入loss（对logits分数加一个threshold）



# ELBO

* $x \to z; q(z) \to p(z|x)$

* main 

  * $KL(q(z)||p(z|x)) $
  * $= E_q[logq(z)]-E_q[logp(z|x)]$

  * $=E_q[logq(z)]-E_q[logp(x,z)]+logp(x)$

  * $=-ELBO+logp(x)$

* things to know
  * $logp(x)-ELBO=KL \ge0$
  * $KL = KL(q(z)||p(z|x))$
  * $ELBO$
  * $= E_q[logq(z)]-E_q[logp(x,z)]$
  * $= E_q[logp(x|z)]-KL(q(z)||p(z))$ （更方便计算，注意KL等于两个期望的差）
* 优化logp可以优化ELBO
* 优化KL就是优化ELBO
