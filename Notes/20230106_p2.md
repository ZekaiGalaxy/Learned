# MIST : Multi-modal Iterative Spatial-Temporal Transformer for Long-form Video Question Answering

https://arxiv.org/pdf/2212.09522.pdf

#### 任务：

VQA

#### 问题：

现在的attention太宽泛了，把所有的patch都过一个self attention，实际上我们只需要和question有关系的patch

<img src="https://p.ipic.vip/hl510n.png" alt="p2" width="400"/>

<img src="https://p.ipic.vip/fo7ufn.png" alt="p2" width="400"/>

<img src="https://p.ipic.vip/5nbvpk.png" alt="p2" width="400"/>

#### 他的方法：

* 先抽取特征，有k个segment，通过和q的attention进行**differentiable attention topk**
* 然后再对segment里面的region进行同样操作
* 不同质的信息也可以token拼接起来，然后进行self attention，但是关键要有type embedding
* cascaded**, 筛选需要的地方，可以用层级的方式**
* **differentiable topk : Gumbel softmax**
  * max:  $x^*=argmax(h)$
  * Gumbel:  $x^* \sim softmax_{\tau}(log(p)+g),g:noise,p:softmax(h)$
  * $g=-log(-log(\epsilon)),\epsilon \sim U(0,1)$
    * Proof the form of g
    * 如果选择x_i，那么 $log(p_i)-log(-log(\epsilon_i)) > log(p_j)-log(-log(\epsilon_j))$  , $\epsilon_k < \epsilon_i^{p_k/p_i}$
    * $P = \Pi \epsilon_i^{p_k/p_i} =\epsilon_i^{1/p_i-1}$
    * 0-1积分一下就是p_i
  * 我们可以控制温度来逼近one-hot

```
https://gist.github.com/rahular/6091da25c8c8ce32f6310ec7399a135b
eps = 1e-20
  
def gumbel(logits):
	"""Draw a sample from the Gumbel-Softmax distribution."""
	u = torch.rand(logits.shape)
	z = -torch.log(-torch.log(u+eps)+eps)
	return logits + z
	
def topk_sample(logits, k=0, t=0.):
	logits = gumbel(logits)
	khot = torch.zeros(logits.shape)
	onehot_approx = torch.zeros(logits)
	
	# every time do gumbel softmax, mask the selected, continue
	for i in range(k):
		mask = torch.max(1.0-onehot_approx, eps)
		logits += torch.log(mask)
		onehot_approx = torch.nn.softmax(logits / t, dim=-1)
		khot = khot + onehot_approx
		
	return khot

logits = torch.nn.log_softmax(torch.random.normal([2, 10])) 
soft_mask = topk_sample(logits, k=3, t=0.1)
```

