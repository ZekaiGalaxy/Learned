# Neural Machine Translation with Contrastive Translation Memories

https://arxiv.org/pdf/2212.03140.pdf

#### 任务：

NMT with memory

#### 一般方法：

* 把最不常用的存下来
* 用graph based memory
* 直接拼接
* 把retrieval和generation分开来一起训练
* 更finegrained token level

#### 他的方法：

<img src="https://p.ipic.vip/4vwe4q.png" alt="p2" width="700"/>

* 一般都是用greedy retrieval的方式，retrieve最相似的
* 但是可能retreive出来的东西之间也很相似，所以占用很多memory但重复太多没什么用
* 所以应该diverse，让retrieve出来的relevance高，redudancy少
  * 这里除了之间的matrix，介绍另一种方式：
  * $argmax(sim(x,x_i)-\alpha sim(x_i,x_j))$，最大化和结果的相似度，最小化两两之间的相似度
  * 对于相似度，我们其实选择一个word level：
  * $sim_{edit}=\frac{D_{edit}(x,x')}{max(L(x),L(x'))}$
  * 这里还可以引入contrastive loss，对于sentence level repr，我们可以contrastive
  * 推近自己和答案之间的距离，彼此之间互相推远（其实就是逼近均匀分布） 
* GCN
  * 有多个memory的时候，我们该怎么整合？
  * 我们可以引进一个supernode，然后把这个supernode作为最终的repr
  * 用attention的方法组建起来，其实就相当于是hierachical
* 怎么把一个hidden整合到decoder里？
  * cross attention
  * 可以在最后加一个copy module
  * 可以用最后一层的attention score拿过来做gate，然后用vocab injection做
