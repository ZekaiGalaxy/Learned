# Mutual Information Alleviates Hallucinations in Abstractive Summarization

https://arxiv.org/pdf/2210.13210.pdf

### 任务：

减少无中生有的内容

* 无中生有的内容往往发生在我们的模型对下一个token不是很确定的时候，这时候就会随便瞎猜，由于模型更倾向于生成frequency大的，于是无中生有的内容也一般是生成概率最大的那些
* 目标很明确：在模型不确定的情况下，惩罚那些生成概率大的东西，希望模型尽可能地生成和context有关的东西
  * 不确定：熵大的时候 $H(y|y_{t},x)>\tau$
  * 和context有关，用mutual information
    * $MI=logp(y|x)-logp(y)$，其实第二项就是惩罚那些生成概率大的东西
  * 合并起来
    * $score = logp(y|x) - \lambda 1_{[H(y|y_{t},x)>{\tau}]} logp(y) $
    * CPMI conditional pointwise mutual information decoding
