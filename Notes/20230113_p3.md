# Keywords and Instances: A Hierarchical Contrastive Learning Framework Unifying Hybrid Granularities for Text Generation

https://arxiv.org/pdf/2205.13346.pdf

#### 任务：

利用contrastive loss进行更好的generation

#### 他的方法：

<img src="https://p.ipic.vip/5c6ml5.png" alt="p2" width="700"/>

平常的contrastive都是sentence level，他提出要multi granularity，设计一个word level的

* word选择keyword，然后对于每个(x,y)，我们可以把它们之间的keyword作为正样例，然后把其他的keyword作为负样例
* 对于keyword，我们可以建图来得到它们的hidden
* 我们原来对于两个分布之间有一些loss，比如KLloss，其实对点和分布之间也有loss
  * point- distribution loss ： Mahalanobis distance
* VAE and CVAE
  * 最本质的东西是什么？
    * 我们想通过一个distribution进行采样来生成东西，一般都是Gaussian
    * $z \sim N(0,I) \to X$, which X？我们需要z的分布是由某个X生成的，而不是所有的X
    * 所以 $z \sim P(Z|X) \to X$, 在inference阶段我们直接从N(0,I)采样，因此要保证的是 $P(Z)=\sum P(Z|X)P(X) \sim N(0,I)$, 其实我们只需要保证每个P(Z|X)是gaussian就可以保证这一点
    * 因此总流程：
      * $X_k \to \mu_k,\sigma_k \to P(Z|X_k) \to X_k$
      * 我们监督的是reconstruction
      * 但是这样train起来会有问题：慢慢的模型找到了捷径：可以让sigma变成0，这样更好，所以我们要regularize
      * $KL(P(Z|X_k)||N(0,I))$
      * 而所谓的CVAE就是
        * $P(Z|X_k) \to P(Z|X_k,Y); KL(P(Z|X_k)||N(0,I)) \to KL(P(Z|X_k,Y)||N(0,I))$
  * 一些数学表示
    * prior distribution $p_\theta(z)$, input $x$, latent $z$
    * $z \sim p_\theta(z|x), x^*=p_\theta(x|z)$
    * $p_\theta(z|x)$ unknown, use approx $q_\phi(z|x)$
    * $L=E_{z \sim q_\phi(z|x)}[log p_\theta(x|z)] - D_{KL}(q_\phi(z|x)||p_\theta(z))$
    * CVAE:
    * $L=E_{z \sim q_\phi(z|x,y)}[log p_\theta(x|z,y)] - D_{KL}(q_\phi(z|x,y)||p_\theta(z|y))$
    * 代表着每个标签y都可以有不同的prior distribution $p_\theta(z|y)$
