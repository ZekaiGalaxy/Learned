# Mixture of Attention Heads: Selecting Attention Heads Per Token

https://arxiv.org/pdf/2210.05144.pdf

### 任务：

efficient transformer

### 他的方法：

* 像是MoE一样，我们要的是MoA，每个hidden不再全部输入下一步，而是筛选出topk个token来完成
* 这里有个小trick，让这种不连续的筛选更加smooth
  * $p_i'=\frac{p_i}{detach(\sum p_j)}$，我们把分母作为常量detach掉，不回传梯度，这样就更加稳定了
