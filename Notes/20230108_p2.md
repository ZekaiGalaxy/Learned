# The Importance of Being Parameters: An Intra-Distillation Method for Serious Gains

https://arxiv.org/pdf/2205.11416.pdf

#### 任务：

knowledge distillation

#### 别人方法：

比如说model pruning，把没用的参数去掉，因为salience score分布差异很大，但是有可能每个参数都有用，我们要求的是每个参数的salience score分布一致，更balanced，这样就有更好的performance

#### 他的方法：

Intra-distillation: balance parameter sensitivity

<img src="https://p.ipic.vip/oiaw32.png" alt="p2" width="400"/>

* "winning tickets": high contribution params
* 选择k组参数，模型forwardk次，让这k个结果分布差不多，从而达到balanced
  * 在实际操作的时候，用的是dropout and mask
* define sensitivity
  * $\mathcal{I(x)=|L(\Theta)-L(\Theta_{-x})| = |\Theta_x^T \partial_x L(\Theta)|}$
* self distillation for more balanced params
  * k组logit的KL divergence叫 X-divergence
  * 以JS div（对称）为基础
  * $L_{x-div}=\sum KL(p||\hat p)+KL(\hat p||p)$
* adaptive intra distillation

<img src="https://p.ipic.vip/crs8i1.png" alt="p2" width="400"/>

<img src="https://p.ipic.vip/fk67y3.png" alt="p2" width="400"/>

