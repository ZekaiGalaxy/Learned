# Enhancing Dual-Encoders with Question and Answer Cross-Embeddings for Answer Retrieval

https://arxiv.org/pdf/2206.02978.pdf

#### 任务：

也是answer retrieval，但是前面方法有个问题，你的answer hidden被映射到question，你就不能又映射成answer

generative != question-answer

#### 他的方法：

* 用cross encoder的空间向量来蒸馏给正常的dual encoder

<img src="https://p.ipic.vip/necohf.png" alt="p2" width="700"/>

* retrieval loss

  * Batch-wise，这不是contrastive loss，要区分开来！！！
  * batch里有几对(q,a)
  * $L_{dual}=-\frac{1}{B}\sum \limits_{1}^{B}log\frac{exp(a_i,q_i)}{\sum \limits_{1}^{B}exp(q_i,q_j)}$

* Cross Attention

  * 把question的知识引入answer的attention中
  * $H_q \in R^{N \times d}, H_a \in R^{M \times d}$
  * 我们可以用q去a的hidden里attention一圈，得到的是q在a空间里的表示，然后aggregate到q空间里，这样就完成了interaction
  * $H_{attnq}=softmax(\frac{(W_qH_q)(W_kH_a)^T}{\sqrt{d_k}})W_vH_a$

* 利用cross encoder的向量来和dual encoder的向量对齐

  * 一般来说，向量对齐可以用mse，但是too hard！

    * Geometry Alignment! 在几何上对齐（利用邻居之间的关系对齐）

    * $p(e_i|e_j)$可以得到一个matrix，为了更soft，我们不用mse，而是KL div

    * $L_{a|q} = p_{cross}(a|q)log(\frac{p_{cross}(a|q)}{p_{dual}(a|q)})$

    * 接下来model $p(e_i|e_j)$

      * $\frac{exp(-d(e_i,e_j))}{\sum exp(-d(e_i,e_j))}$
      * $d(e_i,e_j)= exp(-\frac{||e_ie_j^T||}{\sigma})$, Gaussian

      
