# Fine- and Coarse-Granularity Hybrid Self-Attention for Efficient BERT

https://arxiv.org/pdf/2203.09055.pdf

### 任务：

efficient transformer，主要是对attention进行优化

### 他的方法：

<img src="https://p.ipic.vip/lxw960.png" alt="p2" width="600"/>

在attention过程中，分为informative token和uninformative ones，我们可以对后者的信息进行压缩

* 只对self attention进行优化，所以我们可以用attention score来判断informativeness
  * 直接对attention score进行相加，然后head平均
* informative的原封不动，不informative的直接平均，也可以用attention score加权
