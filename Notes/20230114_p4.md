# Combining Curriculum Learning and Knowledge Distillation for Dialogue Generation

https://aclanthology.org/2021.findings-emnlp.111.pdf

#### 任务：

在对话生成的知识蒸馏中考虑CL

#### 他的方法：

* data上，按照长短，CE分成k类不同难度的，排序喂给模型
* model上，用了一个adversarial的方法
  * 可以把discriminator作为difficulty的分数
  * 那么我们可以用self-paced learning:
    * 每次训练的数据是难度小于等于lambda的构成的集合



# 
