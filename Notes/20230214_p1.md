# Sparse Teachers Can Be Dense with Knowledge

https://arxiv.org/pdf/2210.03923.pdf

### 任务：

KD

### 他的方法：

先进行trial distillation，然后可以得到一些参数是否适合蒸馏（它的saliency和student friendliness）

* 怎么衡量params的这些属性？我们可以先设置一个值，一开始为1.0，然后我们可以对这些值求导，从而得出相关属性
* $MHA=\sum \lambda_i attn$, $FFN=gelu(XW_1)diag(v)W_2$
* 我们通过不同loss对这些参数求导的norm来反应属性
  * 比如sensitivity就是task loss的求导
  * 然后friendliness就是KD loss的求导
