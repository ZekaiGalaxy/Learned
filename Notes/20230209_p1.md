# Improved Knowledge Distillation for Pre-trained Language Models via Knowledge Selection

https://arxiv.org/pdf/2302.00444.pdf

### 任务：

KD

### 他的方法：

* 分成不同质的knowledge，然后不同的加权
  * ResK
    * logit
  * FeaK
    * intermediate
  * RelK
    * relationship bewtween layers
  * FinK
    * task
