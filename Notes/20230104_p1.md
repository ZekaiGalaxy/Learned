# Decoupled Knowledge Distillation

https://arxiv.org/pdf/2203.08679.pdf

#### 任务：

KD

#### 一般方法：

logit KD: $KL(p_T||p_S)$

#### 他的方法：

* 分成target class和非target class，TCKD，NCKD
* 一般的方法 $KD=TCKD+(1-p^T)NCKD$
* TCKD代表了“难度”，$p^T$代表了teacher对知识的confidence，KD混为一谈是不对的
* confidence越大，应该权值越大；NCKD代表了dark knowledge，错误分类的分布才是知识

<img src="https://p.ipic.vip/64lzgg.png" alt="p2" width="400"/>
