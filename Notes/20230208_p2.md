# Feed-Forward Blocks Control Contextualization in Masked Language Models

https://arxiv.org/pdf/2302.00456.pdf

### 任务：

研究transformer结构里的ffn

### 他的方法：

* 利用attribution map
  * 主要是看某个x对y的contribution，相当于attention map
* 由于attention特殊的性质，其实我们光研究attribution的大小是不够的
  * 我们要的是attribution的大小*向量的norm
  * 因为我们认为真正有信息的不是大小，而是向量的方向
  * 那么这样来看ffn其实是很有价值的，因为它可以轻易的改变向量的长度，从而就是改变hidden在某一些平面代表的信息量
* 怎么比较attribution map？
  * 我们可以pool成长条状然后speareman
  * IG：integrated gradient
    * $IG(x)=(x-x') \int_{\alpha=0}^1 \frac{\partial F(x'+\alpha(x-x'))}{\partial x_i}$
  * http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf

<img src="https://p.ipic.vip/fw11t0.png" alt="p2" width="400"/>
