# Compressing Visual-linguistic Model via Knowledge Distillation

https://arxiv.org/pdf/2104.02096.pdf

#### 任务：

knowledge distillation

#### 他的方法：

* hidden repr distillation
  * $MSE(h_{student}W,h_{teacher})$
  * noise contrastive estimation (NCE) loss
    * negative example : random sampled teacher embedding
    * positive example : right teacher embedding
