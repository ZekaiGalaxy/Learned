#### Maria: A Visual Experience Powered Conversational Agent
https://arxiv.org/pdf/2105.13073.pdf  
20230101_p1

#### Multimodal Dialogue Response Generation
https://arxiv.org/pdf/2110.08515.pdf    
20230101_p2

#### Neural Module Networks
20230102_p1

#### Lite Unified Modeling for Discriminative Reading Comprehension
https://arxiv.org/pdf/2203.14103.pdf      
20230102_p2

#### Synthetic Question Value Estimationfor Domain Adaptation of Question Answering
https://arxiv.org/pdf/2203.08926.pdf      
20230102_p3

#### Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning
https://arxiv.org/pdf/2109.05687.pdf    
20230102_p4

#### NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better
https://arxiv.org/pdf/2202.12024.pdf    
20230102_p5

#### Persona-Guided Planning for Controlling the Protagonistâ€™s Persona in Story Generation
https://arxiv.org/pdf/2204.10703.pdf    
20230102_p6

#### Prefix Language Models are Unified Modal Learners (Davinci)
https://arxiv.org/pdf/2206.07699.pdf    
20230102_p7

#### Compressing Visual-linguistic Model via Knowledge Distillation
https://arxiv.org/pdf/2104.02096.pdf    
20230102_p8

#### EfficientVLM: Fast and Accurate Vision-Language Models via Knowledge Distillation and Modal-adaptive Pruning
https://arxiv.org/pdf/2210.07795.pdf    
20230102_p9

#### Increasing faithfulness in knowledge-grounded dialogue with controllable features
https://arxiv.org/pdf/2107.06963.pdf    
20230103_p1

#### Retrieval-Free Knowledge-Grounded Dialogue Response Generation with Adapters
https://arxiv.org/pdf/2105.06232.pdf      
20230103_p2

#### Generate, Discriminate and Contrast: A Semi-Supervised Sentence Representation Learning Framework
https://arxiv.org/pdf/2210.16798.pdf      
20230103_p3

#### Contrastive Learning with Prompt-derived Virtual Semantic Prototypes for Unsupervised Sentence Embedding
https://arxiv.org/pdf/2211.03348.pdf      
20230103_p4

#### Ranking-Enhanced Unsupervised Sentence Representation Learning
https://arxiv.org/pdf/2209.04333.pdf      
20230103_p5

#### Decoupled Knowledge Distillation
https://arxiv.org/pdf/2203.08679.pdf      
20230104_p1

#### Knowledge distillation: A good teacher is patient and consistent
https://arxiv.org/pdf/2106.05237.pdf      
20230104_p2

#### GENERATING SEQUENCES BY LEARNING TO SELF-CORRECT
https://arxiv.org/pdf/2211.00053.pdf      
20230104_p3

#### Crossing Variational Autoencoders for Answer Retrieval
https://arxiv.org/pdf/2005.02557.pdf      
20230104_p4

#### Enhancing Dual-Encoders with Question and Answer Cross-Embeddings for Answer Retrieval
https://arxiv.org/pdf/2206.02978.pdf      
20230104_p5

#### Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization
https://arxiv.org/pdf/2010.01672.pdf        
20230105_p1

#### Topic-Aware Contrastive Learning for Abstractive Dialogue Summarization
https://arxiv.org/pdf/2109.04994.pdf        
20230105_p2

#### Topic-Aware Multi-turn Dialogue Modeling*
https://arxiv.org/pdf/2009.12539.pdf        
20230105_p3

#### Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence
https://arxiv.org/pdf/2210.13701.pdf        
20230105_p4

#### DGST: a Dual-Generator Network for Text Style Transfer
https://arxiv.org/pdf/2010.14557.pdf        
20230105_p5

#### So Different Yet So Alike! Constrained Unsupervised Text Style Transfer
https://arxiv.org/pdf/2205.04093.pdf        
20230105_p6

#### Natural Logic-guided Autoregressive Multi-hop Document Retrieval for Fact Verification
https://arxiv.org/pdf/2212.05276.pdf
20230106_p1

#### MIST : Multi-modal Iterative Spatial-Temporal Transformer for Long-form Video Question Answering
https://arxiv.org/pdf/2212.09522.pdf    
20230106_p2

#### LEARNING MULTIMODAL DATA AUGMENTATION IN FEATURE SPACE
https://arxiv.org/pdf/2212.14453.pdf    
20230107_p1

#### Topic Segmentation Model Focusing on Local Context
https://arxiv.org/pdf/2301.01935v1.pdf    
20230107_p2

#### Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space
https://arxiv.org/pdf/2203.14680.pdf    
20230108_p1

#### The Importance of Being Parameters: An Intra-Distillation Method for Serious Gains
https://arxiv.org/pdf/2205.11416.pdf    
20230108_p2

#### Interpreting Language Models with Contrastive Explanations
https://arxiv.org/pdf/2202.10419.pdf    
20230108_p3

#### Linearizing Transformer with Key-Value Memory
https://arxiv.org/pdf/2203.12644.pdf    
20230109_p1

#### HYDRASUM: Disentangling Style Features in Text Summarization with Multi-Decoder Models
https://arxiv.org/pdf/2110.04400.pdf    
20230109_p2

#### A Unified Encoder-Decoder Framework with Entity Memory
https://arxiv.org/pdf/2210.03273.pdf    
20230109_p3

#### Fine-grained Contrastive Learning for Relation Extraction
https://arxiv.org/pdf/2205.12491.pdf    
20230110_p1

#### Curriculum Prompt Learning with Self-Training for Abstractive Dialogue Summarization
http://gerard.demelo.org/papers/dialogue-summarization.pdf    
20230110_p2

#### Multilingual Machine Translation with Hyper-Adapters
https://arxiv.org/pdf/2205.10835.pdf    
20230110_p3

#### On the Transformation of Latent Space in Fine-Tuned NLP Models
https://arxiv.org/pdf/2210.12696.pdf    
20230110_p4

#### Retrieval as Attention: End-to-end Learning of Retrieval and Reading within a Single Transformer
https://arxiv.org/pdf/2212.02027.pdf    
20230110_p5

#### Sentence Representation Learning with Generative Objective rather than Contrastive Objective
https://arxiv.org/pdf/2210.08474.pdf    
20230111_p1

#### A Contrastive Framework for Learning Sentence Representations from Pairwise and Triple-wise Perspective in Angular Space
https://aclanthology.org/2022.acl-long.336v2.pdf    
20230111_p2

#### A Model-Agnostic Data Manipulation Method for Persona-based Dialogue Generation
https://arxiv.org/pdf/2204.09867.pdf    
20230111_p3

#### A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation
https://arxiv.org/pdf/2104.08704.pdf    
20230111_p4

#### ABC: Attention with Bounded-Memory Control
https://arxiv.org/pdf/2110.02488.pdf    
20230111_p5

#### Achieving Conversational Goals with Unsupervised Post-hoc Knowledge Injection
https://arxiv.org/pdf/2203.11399.pdf    
20230111_p6

#### Adapting Coreference Resolution Models through Active Learning
https://aclanthology.org/2022.acl-long.519.pdf    
20230111_p7

#### An Effective and Efficient Entity Alignment Decoding Algorithm via Third-Order Tensor Isomorphism
https://aclanthology.org/2022.acl-long.405.pdf    
20230111_p8

#### Answer-level Calibration for Free-form Multiple Choice Question Answering
https://aclanthology.org/2022.acl-long.49.pdf   
20230112_p1

#### Answering Open-Domain Multi-Answer Questions via a Recall-then-Verify Framework
https://aclanthology.org/2022.acl-long.128.pdf    
20230112_p2

#### Attention Temperature Matters in Abstractive Summarization Distillation
https://arxiv.org/pdf/2106.03441.pdf    
20230112_p3

#### CAKE: A Scalable Commonsense-Aware Framework For Multi-View Knowledge Graph Completion
https://arxiv.org/pdf/2202.13785.pdf    
20230112_p4

#### ADAM: Dense Retrieval Distillation with Adaptive Dark Examples
https://arxiv.org/pdf/2212.10192.pdf    
20230113_p1

#### Neural Machine Translation with Contrastive Translation Memories
https://arxiv.org/pdf/2212.03140.pdf    
20230113_p2

#### Keywords and Instances: A Hierarchical Contrastive Learning Framework Unifying Hybrid Granularities for Text Generation
https://arxiv.org/pdf/2205.13346.pdf    
20230113_p3

#### Finding the Dominant Winning Ticket in Pre-Trained Language Models
https://aclanthology.org/2022.findings-acl.115.pdf    
20230113_p4

#### ProphetChat: Enhancing Dialogue Generation with Simulation of Future Conversation
https://aclanthology.org/2022.acl-long.68.pdf   
20230113_p5

#### Multi-Granularity Structural Knowledge Distillation for Language Model Compression
https://aclanthology.org/2022.acl-long.71.pdf   
20230113_p6

#### Learning to Express in Knowledge-Grounded Conversation
https://arxiv.org/pdf/2204.05805.pdf    
20230114_p1

#### Things not Written in Text: Exploring Spatial Commonsense from Visual Signals
https://arxiv.org/pdf/2203.08075.pdf    
20230114_p2

#### GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval
https://arxiv.org/pdf/2204.08241.pdf    
20230114_p3

#### Combining Curriculum Learning and Knowledge Distillation for Dialogue Generation
https://aclanthology.org/2021.findings-emnlp.111.pdf    
20230114_p4

#### Enhancing the Open-Domain Dialogue Evaluation in Latent Space
https://aclanthology.org/2021.findings-acl.432.pdf    
20230114_p5



