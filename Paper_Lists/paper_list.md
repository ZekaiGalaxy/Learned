#### Maria: A Visual Experience Powered Conversational Agent
https://arxiv.org/pdf/2105.13073.pdf  
20230101_p1

#### Multimodal Dialogue Response Generation
https://arxiv.org/pdf/2110.08515.pdf    
20230101_p2

#### Neural Module Networks
20230102_p1

#### Lite Unified Modeling for Discriminative Reading Comprehension
https://arxiv.org/pdf/2203.14103.pdf      
20230102_p2

#### Synthetic Question Value Estimationfor Domain Adaptation of Question Answering
https://arxiv.org/pdf/2203.08926.pdf      
20230102_p3

#### Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning
https://arxiv.org/pdf/2109.05687.pdf    
20230102_p4

#### NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better
https://arxiv.org/pdf/2202.12024.pdf    
20230102_p5

#### Persona-Guided Planning for Controlling the Protagonistâ€™s Persona in Story Generation
https://arxiv.org/pdf/2204.10703.pdf    
20230102_p6

#### Prefix Language Models are Unified Modal Learners (Davinci)
https://arxiv.org/pdf/2206.07699.pdf    
20230102_p7

#### Compressing Visual-linguistic Model via Knowledge Distillation
https://arxiv.org/pdf/2104.02096.pdf    
20230102_p8

#### EfficientVLM: Fast and Accurate Vision-Language Models via Knowledge Distillation and Modal-adaptive Pruning
https://arxiv.org/pdf/2210.07795.pdf    
20230102_p9

#### Increasing faithfulness in knowledge-grounded dialogue with controllable features
https://arxiv.org/pdf/2107.06963.pdf    
20230103_p1

#### Retrieval-Free Knowledge-Grounded Dialogue Response Generation with Adapters
https://arxiv.org/pdf/2105.06232.pdf      
20230103_p2

#### Generate, Discriminate and Contrast: A Semi-Supervised Sentence Representation Learning Framework
https://arxiv.org/pdf/2210.16798.pdf      
20230103_p3

#### Contrastive Learning with Prompt-derived Virtual Semantic Prototypes for Unsupervised Sentence Embedding
https://arxiv.org/pdf/2211.03348.pdf      
20230103_p4

#### Ranking-Enhanced Unsupervised Sentence Representation Learning
https://arxiv.org/pdf/2209.04333.pdf      
20230103_p5

#### Decoupled Knowledge Distillation
https://arxiv.org/pdf/2203.08679.pdf      
20230104_p1

#### Knowledge distillation: A good teacher is patient and consistent
https://arxiv.org/pdf/2106.05237.pdf      
20230104_p2

#### GENERATING SEQUENCES BY LEARNING TO SELF-CORRECT
https://arxiv.org/pdf/2211.00053.pdf      
20230104_p3

#### Crossing Variational Autoencoders for Answer Retrieval
https://arxiv.org/pdf/2005.02557.pdf      
20230104_p4

#### Enhancing Dual-Encoders with Question and Answer Cross-Embeddings for Answer Retrieval
https://arxiv.org/pdf/2206.02978.pdf      
20230104_p5

#### Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization
https://arxiv.org/pdf/2010.01672.pdf        
20230105_p1

#### Topic-Aware Contrastive Learning for Abstractive Dialogue Summarization
https://arxiv.org/pdf/2109.04994.pdf        
20230105_p2

#### Topic-Aware Multi-turn Dialogue Modeling*
https://arxiv.org/pdf/2009.12539.pdf        
20230105_p3

#### Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence
https://arxiv.org/pdf/2210.13701.pdf        
20230105_p4

#### DGST: a Dual-Generator Network for Text Style Transfer
https://arxiv.org/pdf/2010.14557.pdf        
20230105_p5

#### So Different Yet So Alike! Constrained Unsupervised Text Style Transfer
https://arxiv.org/pdf/2205.04093.pdf        
20230105_p6
