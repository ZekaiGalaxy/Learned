#### Maria: A Visual Experience Powered Conversational Agent
https://arxiv.org/pdf/2105.13073.pdf  
20230101_p1

#### Multimodal Dialogue Response Generation
https://arxiv.org/pdf/2110.08515.pdf    
20230101_p2

#### Neural Module Networks
20230102_p1

#### Lite Unified Modeling for Discriminative Reading Comprehension
https://arxiv.org/pdf/2203.14103.pdf      
20230102_p2

#### Synthetic Question Value Estimationfor Domain Adaptation of Question Answering
https://arxiv.org/pdf/2203.08926.pdf      
20230102_p3

#### Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning
https://arxiv.org/pdf/2109.05687.pdf    
20230102_p4

#### NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better
https://arxiv.org/pdf/2202.12024.pdf    
20230102_p5

#### Persona-Guided Planning for Controlling the Protagonistâ€™s Persona in Story Generation
https://arxiv.org/pdf/2204.10703.pdf    
20230102_p6

#### Prefix Language Models are Unified Modal Learners (Davinci)
https://arxiv.org/pdf/2206.07699.pdf    
20230102_p7

#### Compressing Visual-linguistic Model via Knowledge Distillation
https://arxiv.org/pdf/2104.02096.pdf    
20230102_p8

#### EfficientVLM: Fast and Accurate Vision-Language Models via Knowledge Distillation and Modal-adaptive Pruning
https://arxiv.org/pdf/2210.07795.pdf    
20230102_p9

#### Increasing faithfulness in knowledge-grounded dialogue with controllable features
https://arxiv.org/pdf/2107.06963.pdf    
20230103_p1

#### Retrieval-Free Knowledge-Grounded Dialogue Response Generation with Adapters
https://arxiv.org/pdf/2105.06232.pdf      
20230103_p2

#### Generate, Discriminate and Contrast: A Semi-Supervised Sentence Representation Learning Framework
https://arxiv.org/pdf/2210.16798.pdf      
20230103_p3

#### Contrastive Learning with Prompt-derived Virtual Semantic Prototypes for Unsupervised Sentence Embedding
https://arxiv.org/pdf/2211.03348.pdf      
20230103_p4

#### Ranking-Enhanced Unsupervised Sentence Representation Learning
https://arxiv.org/pdf/2209.04333.pdf      
20230103_p5

#### Decoupled Knowledge Distillation
https://arxiv.org/pdf/2203.08679.pdf      
20230104_p1

#### Knowledge distillation: A good teacher is patient and consistent
https://arxiv.org/pdf/2106.05237.pdf      
20230104_p2

#### GENERATING SEQUENCES BY LEARNING TO SELF-CORRECT
https://arxiv.org/pdf/2211.00053.pdf      
20230104_p3

#### Crossing Variational Autoencoders for Answer Retrieval
https://arxiv.org/pdf/2005.02557.pdf      
20230104_p4

#### Enhancing Dual-Encoders with Question and Answer Cross-Embeddings for Answer Retrieval
https://arxiv.org/pdf/2206.02978.pdf      
20230104_p5

#### Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization
https://arxiv.org/pdf/2010.01672.pdf        
20230105_p1

#### Topic-Aware Contrastive Learning for Abstractive Dialogue Summarization
https://arxiv.org/pdf/2109.04994.pdf        
20230105_p2

#### Topic-Aware Multi-turn Dialogue Modeling*
https://arxiv.org/pdf/2009.12539.pdf        
20230105_p3

#### Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence
https://arxiv.org/pdf/2210.13701.pdf        
20230105_p4

#### DGST: a Dual-Generator Network for Text Style Transfer
https://arxiv.org/pdf/2010.14557.pdf        
20230105_p5

#### So Different Yet So Alike! Constrained Unsupervised Text Style Transfer
https://arxiv.org/pdf/2205.04093.pdf        
20230105_p6

#### Natural Logic-guided Autoregressive Multi-hop Document Retrieval for Fact Verification
https://arxiv.org/pdf/2212.05276.pdf
20230106_p1

#### MIST : Multi-modal Iterative Spatial-Temporal Transformer for Long-form Video Question Answering
https://arxiv.org/pdf/2212.09522.pdf    
20230106_p2

#### LEARNING MULTIMODAL DATA AUGMENTATION IN FEATURE SPACE
https://arxiv.org/pdf/2212.14453.pdf    
20230107_p1

#### Topic Segmentation Model Focusing on Local Context
https://arxiv.org/pdf/2301.01935v1.pdf    
20230107_p2

#### Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space
https://arxiv.org/pdf/2203.14680.pdf    
20230108_p1

#### The Importance of Being Parameters: An Intra-Distillation Method for Serious Gains
https://arxiv.org/pdf/2205.11416.pdf    
20230108_p2

#### Interpreting Language Models with Contrastive Explanations
https://arxiv.org/pdf/2202.10419.pdf    
20230108_p3

#### Linearizing Transformer with Key-Value Memory
https://arxiv.org/pdf/2203.12644.pdf    
20230109_p1

#### HYDRASUM: Disentangling Style Features in Text Summarization with Multi-Decoder Models
https://arxiv.org/pdf/2110.04400.pdf    
20230109_p2

#### A Unified Encoder-Decoder Framework with Entity Memory
https://arxiv.org/pdf/2210.03273.pdf    
20230109_p3

#### Fine-grained Contrastive Learning for Relation Extraction
https://arxiv.org/pdf/2205.12491.pdf    
20230110_p1

#### Curriculum Prompt Learning with Self-Training for Abstractive Dialogue Summarization
http://gerard.demelo.org/papers/dialogue-summarization.pdf    
20230110_p2

#### Multilingual Machine Translation with Hyper-Adapters
https://arxiv.org/pdf/2205.10835.pdf    
20230110_p3

#### On the Transformation of Latent Space in Fine-Tuned NLP Models
https://arxiv.org/pdf/2210.12696.pdf    
20230110_p4

#### Retrieval as Attention: End-to-end Learning of Retrieval and Reading within a Single Transformer
https://arxiv.org/pdf/2212.02027.pdf    
20230110_p5


